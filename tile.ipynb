{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_path = r\"<image_path>\"\n",
    "\n",
    "# get color segments clusters\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "def quantize(image_path, n_colors=16, threshold=0.005):\n",
    "    \"\"\"\n",
    "    Quantize image to n colors. If area of the quantized color is less than 0.5% of the total image area, it is replaced with closest color.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    image = np.array(image)\n",
    "    shape = image.shape\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "    image = image.reshape((-1, 3))\n",
    "    clt = KMeans(n_clusters=n_colors)\n",
    "    labels = clt.fit_predict(image)\n",
    "    quant = clt.cluster_centers_.astype(\"uint8\")[labels]\n",
    "    # filter colors by area\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(unique, counts)\n",
    "    # sort by count\n",
    "    print(sorted(zip(unique, counts), key=lambda x: x[1], reverse=True))\n",
    "    quant = quant.reshape(shape).astype(\"uint8\")\n",
    "    quant = cv2.cvtColor(quant, cv2.COLOR_LAB2RGB)\n",
    "    return quant\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_color_segment_areas(image_path, radius_scale=1.0, min_area=1600, min_area_ratio=0.005):\n",
    "    \"\"\"\n",
    "    Get the areas of the color segments in the image.\n",
    "    We use mask to get color-wise mask, then applies contour detection to get the area of the color segment.\n",
    "    We discard the color segments whose area is less than 0.5% of the total image area.\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    pixels = np.array(image)\n",
    "    original_shape = pixels.shape\n",
    "    pixels = pixels.reshape((-1, 3))\n",
    "\n",
    "    # we already have quantized image, so its colors are already clustered\n",
    "    colors = np.unique(pixels, axis=0) # get unique colors\n",
    "    print(\"Unique colors:\", len(colors))\n",
    "    # get the color segments by masks\n",
    "    masks = []\n",
    "    contours_all = {tuple(color.tolist()): [] for color in colors}\n",
    "    for color in colors:\n",
    "        mask = np.all(pixels == color, axis=1)\n",
    "        assert np.sum(mask) > 0, \"Color not found in the image\"\n",
    "        masks.append(mask)\n",
    "        #display(Image.fromarray(mask.reshape(original_shape[:-1]).astype(\"uint8\") * 255))\n",
    "    # get the area of the color segments\n",
    "    for i, mask in enumerate(masks):\n",
    "        # apply contour detection to get the area of the color segment\n",
    "        mask = mask.reshape(original_shape[:-1])\n",
    "        _, thresh = cv2.threshold(mask.astype(\"uint8\") * 255, 127, 255, 0)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = [cnt for cnt in contours if cv2.contourArea(cnt) > max(min_area, min_area_ratio * original_shape[0] * original_shape[1])]\n",
    "        # centers = [np.mean(cnt, axis=0).astype(int) for cnt in contours]\n",
    "        # center can be out of actual contour, so we need to adjust it if its out of contour\n",
    "        centers = [np.mean(cnt, axis=0) for cnt in contours]\n",
    "        centers = [adjust_center(contour_result, center_estimate) for contour_result, center_estimate in zip(contours, centers)]\n",
    "        contours = [np.squeeze(cnt) for cnt in contours]\n",
    "        radiuses = [np.max(np.linalg.norm(cnt - center, axis=1)) for cnt, center in zip(contours, centers)]\n",
    "        # register the color segment area\n",
    "        contours_all[tuple(colors[i])] = (contours, centers, radiuses)\n",
    "    valid_colors = {color: len(contours_all[color][0]) for color in contours_all}\n",
    "    transparent_background = np.zeros(original_shape, dtype=\"uint8\")\n",
    "    for color in contours_all:\n",
    "        if valid_colors[color] > 0:\n",
    "            for cnt, center, radius in zip(contours_all[color][0], contours_all[color][1], contours_all[color][2]):\n",
    "                print(center, radius)\n",
    "                cv2.circle(transparent_background, tuple(center.flatten()), max(1, int(radius_scale * radius)), color, -1)\n",
    "    return Image.fromarray(transparent_background)\n",
    "\n",
    "def adjust_center(contour, center_estimate):\n",
    "    \"\"\"\n",
    "    Adjust the center to be inside the contour.\n",
    "    \"\"\"\n",
    "    center = center_estimate\n",
    "    \n",
    "    if cv2.pointPolygonTest(contour, tuple(center.flatten()), False) < 0:\n",
    "        # center is outside the contour\n",
    "        print(\"Center:\", center)\n",
    "        # get the closest point on the contour\n",
    "        center_coords = center.flatten()\n",
    "        contour_coords_as_array = contour.reshape(-1, 2)\n",
    "        distances = np.linalg.norm(contour_coords_as_array - center_coords, axis=1)\n",
    "        closest_point = contour[np.argmin(distances)]\n",
    "        center = closest_point\n",
    "        # fix to closest integer\n",
    "        center = np.round(center).astype(int)\n",
    "        print(\"Adjusted center:\", center)\n",
    "        assert type(center) == type(center_estimate), f\"Center type mismatch: {type(center)} != {type(center_estimate)}\"\n",
    "    else:\n",
    "        # to integer\n",
    "        center = np.round(center_estimate).astype(int)\n",
    "    return center\n",
    "    \n",
    "    \n",
    "def canny_image(image_path, low=100, high=200, result_path=None):\n",
    "    \"\"\"\n",
    "    Applies the Canny edge detection algorithm to the image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    canny_edge = cv2.Canny(image, low, high)\n",
    "    # to white background\n",
    "    canny_edge = cv2.bitwise_not(canny_edge)\n",
    "    if result_path:\n",
    "        cv2.imwrite(result_path, canny_edge)\n",
    "    return Image.fromarray(canny_edge)\n",
    "\n",
    "def contour_image(image_path, result_path=None):\n",
    "    \"\"\"\n",
    "    Applies the contour detection algorithm to the image.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image_total_pixels = image.shape[0] * image.shape[1]\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 127, 255, 0)\n",
    "    # get outmost contours only\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # filter contours by area\n",
    "    contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 0.005 * image_total_pixels]\n",
    "    image = cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "    if result_path:\n",
    "        cv2.imwrite(result_path, image)\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "# quant = quantize(image_path, n_colors=16)\n",
    "# quant = Image.fromarray(quant)\n",
    "# quant.save(\"quantized.png\")\n",
    "\n",
    "# canny_quant = canny_image(\"quantized.png\", result_path=\"canny_quantized.png\")\n",
    "\n",
    "# canny_quant_contour = contour_image(\"quantized.png\", result_path=\"quantized_contour.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (get_color_segment_areas(\"quantized.png\", radius_scale=0.05, min_area=400, min_area_ratio=0.001))\n",
    "# get count for each color\n",
    "#display(result)\n",
    "\n",
    "# result has black background, we can remove it to be transparent\n",
    "result = result.convert(\"RGBA\")\n",
    "result_data = np.array(result)\n",
    "# apply transparency mask where the color is black\n",
    "result_data[:, :, 3] = np.where(np.all(result_data[:, :, :3] == [0, 0, 0], axis=-1), 0, 255)\n",
    "result = Image.fromarray(result_data)\n",
    "display(result)\n",
    "# now we have filtered the color segments whose area is less than 0.5% of the total image area\n",
    "\n",
    "# merge with the original image\n",
    "original = Image.open(image_path)\n",
    "original = original.convert(\"RGBA\")\n",
    "\n",
    "# overlay the color segments on the original image\n",
    "result = result.resize(original.size)\n",
    "result = result.convert(\"RGBA\") # should be already RGBA\n",
    "overlay = Image.blend(original, result, alpha=0.5)\n",
    "display(overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlay the color segments on the original image\n",
    "base_image = Image.open(image_path)\n",
    "base_image = base_image.convert(\"RGBA\")\n",
    "result = result.convert(\"RGBA\")\n",
    "transparency_mask = Image.new(\"L\", base_image.size, 0)\n",
    "base_image.alpha_composite(result, dest=(0, 0), source=(0, 0))\n",
    "base_image.convert(\"RGB\")\n",
    "display(base_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
